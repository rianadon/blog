#+OPTIONS: H:9
#+EXPORT_FILE_NAME: 2019-03-24-streaming.md
#+TITLE: A guide to streaming H.264 in FRC

* Delete me

For two and a half years, I've been trying to reliably stream H.264 video back to our driver station. Finally, this year, the stream worked reliably. I'm going to go into a bit of detailing how we did this. The post is structured as a tutorial, with some background theory worked in. I've tried to provide links wherever possible to let anyone interested in more theory explore.

So, first I should start off with:

** What's this H.264 thing and why should you want it?

MJPEG and H.264 are both video codecs. They define how to compress video. The de facto standard in FRC right now is [[http://www.theclosedcaptioningproject.com/?p=524][MJPEG]] (transported over HTTP). And for a pretty good reason. It's easy and fast to compress images to JPEG; you don't need very powerful hardware. Plenty of libraries exist to compress images to JPEG, and streaming the video simply requires sending the images immediately one after another over HTTP. You can also display the stream in browsers, dashboards, and just about anything that's not your toaster.

#+CAPTION: This is the caption for the next figure link (or table)
[[../assets/2019-03-27-mjpegh264.png]]

MJPEG compresses each frame individually, making them more fuzzy to save bandwidth. The technical term for this is spatial compression. H.264 uses this in addition to temporal compression, or compressing changes between each frame. Imagine your robot is bricked (I said imagine; I know this would never happen to you). In your camera feed you see Team 1072 scoring five points in the hatch scale boiler. How much changed in that image? Not much. An MJPEG steam would send everything visible in the feed again, including the beautiful gray carpet. An H.264 stream would send only the information necessary to locate reconstruct the 1072 bot's movement. You may imagine this would add up to lots of data savings. It does. You can fit a somewhat good-looking 320x240 video at 15 fps into 250 kilobits per second. You could stream four cameras under 1 Mbps.

> If your robot is executing victory spins, you'll see less data saving as the scenery around will be constantly changing. However, there will still be compressible elements moving across your field of view, and in practice there are still substantial reductions in data.

I'd rather not go too far in depth explaining how H.264 and other modern video compression codecs work. There are much better guides which already do this like [[https://sidbala.com/h-264-is-magic/][this introductory one]] and [[https://github.com/leandromoreira/digital_video_introduction#readme][this more in-depth one]]. These are both absolutely gorgeous and I'd recommend you read them both for educational learning and expanding your spiritual intellect. But to explain the codec, it applies to each frame the same [[http://blog.biamp.com/how-chroma-subsampling-works/][chroma subsampling]] JPEG uses (dividing the image into a luminosity plus two color channels then heavily compressing the latter) then divides the compressed frame into subsections called blocks. Every so often the codec sends an I-frame or Intra frame. Opposite of the Latin root /inter-/ (between/among), an /intra-/ (inside/within) frame describes a single frame by itself, described only by itself, not performing any motion or temporal compression. If your video consisted of only I-frames, it would be practically the same as an MJPEG stream. P-frames (predicted from the previous frame) and B-frames (bi-predicted from the previous and following frames), describe frames by the motion of the blocks in adjacent frames. Remember how we divided each compressed frame into blocks? P-frames and B-frames encode vectors describing where each of these blocks approximately moved to. If a pink flamingo suddenly appeared in only one frame, however, it would be impossible to find that flamingo in an adjacent frame and say "Hey the flamingo moved from the top left to the center." Unless your camera is mounted high it shouldn't be hallucinating. Thus, to encode for spontaneous pink flamingos among other new objects in the frame, the codec also sends the difference between the actual frame and the frame predicted by block motions. Since such difference should be small, it can be heavily compressed (with [[https://www.youtube.com/watch?v=Q2aEzeMDHMA][DCT]] for example) and sent very efficiently. H.264 applies some extra compression to these frames, and that's all!

** Technology overview

[[https://www.ffmpeg.org/about.html][FFmpeg]] and [[https://gstreamer.freedesktop.org/documentation/application-development/introduction/gstreamer.html][GStreamer]] are possibly the two largest media frameworks. They're both open source too! Both can do streaming, but I find GStreamer more extensible.

*** GStreamer basics

> This is adapted from GStreamer's [[https://gstreamer.freedesktop.org/documentation/application-development/introduction/basics.html][much better tutorial]], which makes for good reading.

Our end goal here is to take video from a camera and send it (or, if you're confident in your ability, fully send it) to another computer. The process that grabs the video, decodes it, re-encodes it, wraps it up, and ships it is called a *pipeline*. A pipeline is like a blanket statement; it accounts for and encapsulates everything going on.

We have to build our pipeline somehow. Not out of wood, but out of *elements*. An element is like that well-written function every programmer desires to write one day: it performs one specific task. That could be reading from a camera, changing color spaces, encoding to H.264, or sending the stream across the network. All these are elements.

Elements need to be linked together. To accomplish this, each element has one or more *pads*. Data flows into an element's sink pad and out of its source pad. These names may seem reversed. Why are we sending data into the sink and out of the source? Elements that provide media (such as a file reader, ~filesrc~) end with ~src~. These have a single source pad. Elements to which data is sent off (~tcpserversink~) end with ~sink~ and have a single sink pad.

Pads also have data types. The tutorial above gives a great analogy:

#+BEGIN_QUOTE
A pad is similar to a plug or jack on a physical device. Consider, for example, a home theater system consisting of an audio amplifier, a DVD player, and a (silent) video projector. Linking the DVD player to the amplifier is allowed because both devices have audio jacks, and linking the projector to the DVD player is allowed because both devices have compatible video jacks. Links between the projector and the amplifier may not be made because the projector and amplifier have different types of jacks. Pads in GStreamer serve the same purpose as the jacks in the home theater system.
#+END_QUOTE

Some elements support multiple data types to be inputted or outputted. GStreamer determines which data types should be used by a process called **caps negotiation**. Caps is short for capabilities. Each element imposes its own restrictions on caps, so the process can be seen as solving a large system of equations for the correct caps to use. Whew! This system of equations may sometimes have multiple solutions, and GStreamer might pick the right one. To recover from this doomsday fiasco, GStreamer includes a special element ~capsfilter~, which allows you to enforce the type of data which flows through it. It's kind of like casting, only without parentheses.

*** Some example GStreamer pipelines

> This is adapted from this other [[https://gstreamer.freedesktop.org/documentation/tutorials/basic/gstreamer-tools.html][GStreamer tutorial]]. It's also good reading.

Install GStreamer using [[https://gstreamer.freedesktop.org/documentation/installing/index.html][this guide]]. Use the 64-bit version if your computer supports it. Make sure to grab the development installer as you'll need it to use GStreamer with OpenCV. If you don't plan on doing this the runtime installer works too. Once GStreamer is installed and on your path, you'll be able to run the command ~gst-launch-1.0~. This is how you launch a GStreamer pipeline. If your terminal replies that the ~gst-launch-1.0~ command was not found, double check the GStreamer ~bin~ folder has been added to your path.

**** A very basic example

Here's about the most basic of a pipeline you can get:
#+BEGIN_SRC sh
gst-launch-1.0 videotestsrc ! videoconvert ! autovideosink
#+END_SRC

~videotestsrc~ outputs [[https://en.wikipedia.org/wiki/SMPTE_color_bars][SMPTE color bars]] to its source pad. ~autovideosink~ takes input from its sink pad and displays it on your screen. The data types of these two pads includes information on their color space. There may be a chance their caps (capabilities) to not account for the same color space. In this case, ~videoconvert~ converts between the two color spaces. It decides which color spaces to convert between as a result of caps negotiation.

**** More complex examples

This one takes video from a Logitech C920 camera (which outputs H.264-encoded video natively) and streams it over UDP to port 5002:

#+BEGIN_SRC sh
gst-launch-1.0 v4l2src device=/dev/video0 name=pipe_src ! video/x-h264,width=1280,height=720,framerate=15/1,profile=baseline ! h264parse ! rtph264pay pt=96 config-interval=5 ! udpsink name=udpsink0 host=127.0.0.1 port=5002 async=false
#+END_SRC

While this worked in testing, this specific pipeline was unreliable in competition last year. It may have been because it was UDP. Perhaps it was because I wrote my own wrapper around this pipeline to determine the ip address of the driver station (which we didn't make static), and the wrapper had bugs.

The ~video/x-h264~ in the pipeline above is short for a ~capsfilter~. As the camera could output a variety of framerates and frame sizes, we must enforce the size and framerate we want.

The one below takes input from a normal webcam, converts it to h264, (we used some special fisheye ones but anyone which works with your laptop and doesn't require fancy drivers should work) and hands it over to an app:

#+BEGIN_SRC sh
gst-launch-1.0 v4l2src device=/dev/video0 ! video/x-raw,width=320,height=240,framerate=15/1 ! videoconvert ! video/x-raw,format=I420 ! x264enc tune=zerolatency bitrate=250 threads=1 ! rtph264pay config-interval=1 name=pay0 pt=96 ! appsink
#+END_SRC

The ~rtph264pay~ element puts the stream into the RTP container format. H.264 video is basically just a stream of bits. RTP defines how these bits get assembled into packets which can then be sent one at a time over the network.
As for the H.264 encoder, ~x264enc~, the bitrate of 250 is 250 Kbps, and we use one thread because we had four cameras streaming video. If you have a machine with four cores and are using four separate processes for sending video, why bother multithreading even more? The ~tune=zerolatency~ here is a preset that adjusts the parameters of the encoder to try to minimize the amount of time it takes to compress the video. We're aiming for speed rather than quality here.

** GStreamer RTSP Server

I made the lucky choice of using [[http://www.informit.com/articles/article.aspx?p=169578&seqNum=3][RTSP]] this year, for it offers numerous advantages that using a simple ~udpsink~ or something doesn't:

- It's a server. That means you don't need a static ip address for whatever is connecting to it
- It's a server. That means you can connect to it multiple times and don't need to worry about something else hogging the stream.
- It's a server. That means YOU DON'T HAVE TO WRITE YOUR OWN
- It's a server. That means you can negotiate settings with it (read: not having to hardcode them into the server), such as whether you're using TCP or UDP
- The client rtspsrc has a ton of options. Like what port ranges to receive on, etc.
- You can have as many streams as you like controlled on a single port. So you won't chew up the limited number allotted by FIRST.

You may notice I said it's a server. I think that's really cool.

*** Installation

This thing doesn't come with GStreamer. This means you'll have to download and build it yourself. Here is a method that worked for me:

1. Determine the gstreamer version by running ~gst-launch-1.0 --version~. For me that's ~1.12.3~, so I’ll use that in future instructions.
2. Download the tarball of the correct release of the RTSP server. For me this is at http://gstreamer.freedesktop.org/src/gst-rtsp-server/gst-rtsp-server-1.12.3.tar.xz. Note the version number in here. You can use ~wget~ for downloading.
3. Unzip that tarball by executing ~tar -xf gst-rtsp-server-1.12.3.tar.xz~
4. At this point you may delete the tarball
5. Navigate to the source directory (~cd gst-rtsp-server-1.12.3~)
6. Run ~./configure~
7. Compile! Run ~make -j4~ (the ~-j4~ splits this into 4 processes to run on 4 cores, use a different number if you have a more powerful computer)

There should now be lots of little binaries in the ~examples~ folder!

*** My own server pipeline

To get started, you can use the ~test-launch~ binary to play with various pipelines. It takes in the port you want to run the rtsp server on (for exampe 5800 to be compatible with FRC rules) as an argument as well as the GStreamer pipeline you'd like to use. For example:

#+BEGIN_SRC sh
./test-launch -p 5800 "( videotestsrc ! x264enc ! rtph264pay name=pay0 pt=96 )"
#+END_SRC

There are several ways to view this stream. One is to use VLC. Open a network stream at the url ~rtsp://localhost:5800/test~ and you should see some video. While this method works and you may already have VLC installed, it adds significant lag for some reason. I would 96/100 would not recommended for anything except testing that the stream opens correctly.

The other option is to use GStreamer, which works much better. Use the following command to view your stream:

#+BEGIN_SRC sh
gst-launch-1.0 rtspsrc location=rtsp://localhost:5800/test latency=0 ! rtph264depay ! decodebin ! autovideosink
#+END_SRC

Unlike VLC, the stream should launch almost instantly.

If you'd like to be more advanced, you can use one of the pipelines several sections above to take input from your camera, which may or may not be more helpful for FRC than pixels colored in some predetermined pattern. This is one example which takes input from any camera you may have nearby and runs encoding on your computer:

#+BEGIN_SRC sh
./test-launch -p 5800 "( v4l2src device=/dev/video0 ! video/x-raw,width=320,height=240,framerate=15/1 ! videoconvert ! video/x-raw,format=I420 ! x264enc tune=zerolatency bitrate=250 threads=1 ! rtph264pay config-interval=1 name=pay0 pt=96 )"
#+END_SRC

When viewing the stream, you should see a nice-looking grey rectangle. After a few seconds, the H.264 encoder will send an I frame and the video thereafter should be representative of what your camera sees.

*** Some fries on the side

For some, one camera feed is not enough. You might want a camera on the side of your robot, perhaps underneath, or maybe facing up so you don't have to strain your neck to look at the ceiling when you're bored. While this task may seem daunting, don't worry! There's more to the GStreamer RTSP server project than just this one command line example. It's an /example/ after all. To add multiple feeds I'll work off the ~test-launch.c~ example. Here are the changes you'll need to make:

**** 1. Hardcode the pipeline

If you're feeling like a good programmer, you could pass all the pipelines through the command line and support any number of them. I do not identify myself as such a mythical programmer, so I will instead embed the pipelines in the source code.

Replace the line ~gst_rtsp_media_factory_set_launch (factory, argv[1]);~ with something like:

#+BEGIN_SRC c
gst_rtsp_media_factory_set_launch (factory, "( v4l2src device=/dev/video0 ! video/x-raw,width=320,height=240,framerate=15/1 ! videoconvert ! video/x-raw,format=I420 ! x264enc tune=zerolatency bitrate=250 threads=1 ! rtph264pay config-interval=1 name=pay0 pt=96 )");
#+END_SRC

**** 2. Add the second pipeline!

Before the line you just changed, there's a line which creates a ~factory~ object. This line creates an object used to stream our pipeline. The two lines of code below also help accomplish this mission. To add a second pipeline, you'll need to copy these four lines and modify them a little. Change the pipeline string to something different this time and change the url the pipeline is served on.

In total, you should be adding four lines similar to the following directly before the line ~g_object_unref (mounts);~:

#+BEGIN_SRC c
factory = gst_rtsp_media_factory_new ();
gst_rtsp_media_factory_set_launch (factory, "( videotestsrc ! x264enc ! rtph264pay name=pay0 pt=96 )");
gst_rtsp_media_factory_set_shared (factory, TRUE);
gst_rtsp_mount_points_add_factory (mounts, "/fries", factory);
#+END_SRC

Repeat this step as many times as you'd like for more streams!

**** 3. Compile!

Save ~test-launch.c~, open a terminal in the same directory, and run ~make~. If the code compiles sans errors, running ~./test-launch -p 5800~ should start an RTSP server with two streams accessible by ~rtsp://localhost:5800/test~ and ~rtsp://localhost:5800/fries~.

*** Extra fanciness

There's much more to the RTSP server. Examples of these other "much more"s are in the ~example~ folder. Perhaps you are concerned about job security and would like to password protect your stream so you are the only person on the team capable of opening it. The file ~test-auth.c~ has an example of how to implement authentication.

*** Some more on viewing

As mentioned before, the ~rtspsrc~ element comes with a plethora of features. Running ~gst-inspect-1.0 rtspsrc~ will list you them. You can use ~protocols=tcp~ to force the server to send the stream over TCP (which we have not had to do but may be needed if UDP proves unreliable for you) or ~port-range=5800-5810~ to force the stream to be sent back via a limited subset of ports.

> Historically the FMS has seemed to filter only the ports servers are hosted on, which means clients can connect on any port they like. So while the RTSP server has to be on a legal port like 5800, we've been able to connect on any random port number. Thus, we haven't needed to use the ~port-range~ option. But it's there if you ever need it.

** Systemd and services

If you're running GStreamer on Linux, chances are your system comes with a tool for starting services on boot and logging them called [[https://www.linode.com/docs/quick-answers/linux-essentials/what-is-systemd/][systemd]]. While it is [[https://www.zdnet.com/article/linus-torvalds-and-others-on-linuxs-systemd/][controversial]] due primarily to its [[http://without-systemd.org/wiki/index.php/Arguments_against_systemd][feature creep among other issues]], it's neverless utilized by many of the processes your computer runs on boot and in the background. To automatically run our RTSP server on boot, I'll [[https://www.devdungeon.com/content/creating-systemd-service-files][create a systemd service]].

Here is the service file I'll use:

Now instruct systemd to load the service files again:

#+BEGIN_SRC sh
sudo systemctl daemon-reload
#+END_SRC

To start our service and enable it on boot, run:

#+BEGIN_SRC sh
sudo systemctl start streaming
sudo systemctl enable streaming
#+END_SRC

If you run ~sudo systemctl status streaming~, you should see that the service is running and some log output from the server.

If you'd like to view the complete logs of the service, you can run:

#+BEGIN_SRC sh
journalctl -u streaming
#+END_SRC

*** Bonus: Saving limelight video

Systemd is useful for just about any service you want to run. Since we use a [[https://limelightvision.io/][Limelight]] on our robot, we find it useful to save its video for future debugging purposes. Below is a script to do just that:

#+BEGIN_SRC bash
#!/bin/bash
# download-limelight.sh

# Form a filename based on the current date and time
file=videos/$(date "+%Y-%m-%d;%H-%M-%S-limelight.mjpeg")
# Download the stream to that file
gst-launch-1.0 souphttpsrc location=http://10.10.72.11:5802 ! filesink location=$file
#+END_SRC

This is the service file which runs that script:

#+BEGIN_SRC conf
# /etc/systemd/system/download-limelight.service

[Unit]
Description=Limelight video downloader

[Service]
Type=simple
ExecStart=/home/team1072/download-limelight.sh
Restart=always
RestartSec=30

[Install]
WantedBy=multi-user.target
#+END_SRC

The ~RestartSec~ parameter instructs systemd to wait 30 seconds before restarting the script in case it fails. This way, if the Limelight isn't on, the script won't hog any CPU power.

** OpenCV and GStreamer

In the examples above GStreamer was used to view the RTSP stream. This is enough for any casual use, and you can write a batch script (or whatever shell scripting language your system supports) to launch multiple streams. However, there's a limit: GStreamer supports a limited number of options for modifying your stream (such as adding overlays or applying perspective transforms). That's not to say its support is basic: GStreamer can rotate your stream, draw SVGs on top, scale it, and add filters so you can post it on your social media. However, opening the stream via OpenCV drops you into a magical world.

Unfortunately, OpenCV doesn't just come with GStreamer support. The package on pip doesn't support it. The anaconda packages don't support it by default. Perhaps Homebrew supports it with the right options, but the FRC driver station only runs on Windows so Mac software is out of the question. To gain GStreamer support, you'll need to build OpenCV yourself. The process takes some time, so be prepared to set aside a few hours, but after building OpenCV you'll be one FBI visit away from feeling like a hacker. Let's get started!

*** 1. Download GStreamer

If you didn't download the development version of GStreamer, install it using [[https://gstreamer.freedesktop.org/documentation/installing/index.html][this guide]].

*** 2. Download OpenCV

Grab a release from the [[https://github.com/opencv/opencv/releases][OpenCV GitHub releases page]]. Download the archive labeled ~Source code~ in your preferred format. Unzip/tar it to some location you'll remember.

*** 3. Download the build tools

To build OpenCV you can choose between using Visual Studio and MinGW as your compiler. I'll use Visual Studio since it's well supported by Microsoft which made your operating system. Download the community edition from [[https://visualstudio.microsoft.com/downloads/][here]] (or by searching for Visual Studio in your preferred search engine). OpenCV also uses CMake as a build tool. Binary distributions are available on the [[https://cmake.org/download/][CMake download site]].

*** 4. Install everything else

If you'd like to use OpenCV from Python, make sure Python and NumPy (via ~pip install numpy~) are installed. There are several other optimization libraries you may wish to install listed on OpenCV's [[https://docs.opencv.org/3.4.3/d3/d52/tutorial_windows_install.html][tutorial]].

*** 5. Create a build folder

Open the OpenCV folder you downloaded from GitHub. It should have files named ~README.md~ and ~CMakeLists.txt~ in it. Create a new folder inside and name it ~build~.

*** 6. Run CMake

Open the CMake graphical interface. Next to "Where is the source code:", select the OpenCV folder. As for "Where to build the binaries:", select the build folder you created.

Then click "Add entry". Add a path option with name ~GSTREAMER_DIR~ and path ~C:/gstreamer/1.0/x86_64~ (assuming you installed 64-bit GStreamer to ~C:/gstreamer~).

Click the Configure button. You'll be asked to select a compiler. Select the version of Visual Studio you installed. You'll also be asked to select an architecture (Optional platfor for generator). Chances are you installed the 64 bit version of GStreamer, so use ~x64~. If Visual Studio complains about errors linking libraries, chances are you selected the wrong architecture.

If the output below shows that OpenCV found GStreamer, click "Generate" then "Open Project".

*** 7. Build with Visual Studio

After clicking "Open Project", Visual Studio will open itself. Under the solution panel, right click the ~BUILD_ALL~ task and click "Build". This process takes about an hour depending on how powerful your computer is. Once it finishes successfully, click build the ~INSTALL~ task.

*** 8. Prosper

After running Python, you should be able to ~import cv2~. You can open the RTSP stream using a GStreamer pipeline via the OpenCV ~VideoCapture~ object, such as in the following code:

#+BEGIN_SRC python
import cv2

# Open the stream via GStreamer. Note that the pipeline ends with an appsink
# element, which OpenCV reads from. The sync and drop options here instruct
# Gstreamer to not block the program waiting for new frames and to drop
# frames if OpenCV cannot read them quickly enough.
def makecap():
    return cv2.VideoCapture(
        'rtspsrc location=rtsp://localhost:5800/test latency=0 ! rtph264depay ! decodebin ! appsink sync=false drop=true',
        cv2.CAP_GSTREAMER
    )

cap = makecap()
while True:
    successful, frame = cap.read()
    if successful: # Display the frame
        cv2.imshow('Frame', frame)
        if cv2.waitKey(10) == ord('q'):
            break # Exit the program if the key q is pressed
    else: # If a frame can't be read try restarting the stream
        cap = makecap()
#+END_SRC

The code handles auto-restarting the RTSP client, so you can leave the script running in the background and let it automatically reconnect when your RTSP server comes online. Even if you don't have any processing to do to the frame, OpenCV is worth its return in automatic restarting your stream if your network fails.

If you're not running any processing, you may prefer to continue using GStreamer's viewing window. In that case, you can split the video into one stream that's displayed an another which is passed to OpenCV. This as an example pipeline to accomplish this:

#+BEGIN_SRC python
'rtspsrc location=rtsp://10.10.72.12:5800/test latency=0 transport=tcp ! rtph264depay ! decodebin ! tee name=t ! queue ! videoconvert ! appsink sync=false drop=true t. ! queue ! videoconvert ! autovideosink'
#+END_SRC

** Appendix: Some Code

Throughout this tutorial I've been pasting snippets of code. If you'd like to see our code in practice, it's on a GitHub repo I'll paste here soon.
